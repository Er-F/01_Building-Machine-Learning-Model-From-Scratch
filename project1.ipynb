{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy and Gini-index Functions. \n",
    "def H(p):\n",
    "    \"\"\"Calculates entropy from a probability distribution p. \n",
    "\n",
    "    Args:\n",
    "        p ([1D Array]): [Probability Distribution]\n",
    "\n",
    "    Returns:\n",
    "        [Scalar]: [Entropy]\n",
    "    \"\"\"\n",
    "    # Entropy Definition. \n",
    "    H = -np.sum(p*np.log2(p))\n",
    "    return H \n",
    "\n",
    "def H_A(prob_low, dist_low, prob_high, dist_high):\n",
    "    \"\"\"Calculate Entropy change (E(T|A)) given False and Positive observations (A) from a two-\n",
    "    possible outcome space. \n",
    "\n",
    "    Args:\n",
    "        p_low ([1D Array]): [Probability distribution given false observation]\n",
    "        p_high ([1D Array]): [Probability distribution given positive observation]\n",
    "\n",
    "    Returns:\n",
    "        [float]: [Weighted average Entropy E(T|A)]\n",
    "    \"\"\"\n",
    "    # Calculate the Entropy change given observation/non observation. \n",
    "    return prob_low * H(dist_low) + prob_high*H(dist_high)\n",
    "\n",
    "\n",
    "def giniValue(px):\n",
    "    \"\"\"Calculate gini-index for a probability distribution px\n",
    "\n",
    "    Args:\n",
    "        px ([1D-Array]): [Probability distribution.]\n",
    "\n",
    "    Returns:\n",
    "        [float]: [Gini-index for that probability distribution.]\n",
    "    \"\"\"\n",
    "    return 1 - np.sum(px**2)\n",
    "\n",
    "def G_A(prob_low, dist_low,prob_high,dist_high ):\n",
    "    \"\"\"Calculated the weighted average gini value between two vectors, where relative probabilities are known. \n",
    "\n",
    "    Args:\n",
    "        prob_low ([float]): [Overall prob for lower vector]\n",
    "        dist_low ([1D]-Array): [probability distribution for lower vector]\n",
    "        prob_high ([float]): [Overall prob for lower upper vector]\n",
    "        dist_high ([1D-Array]): [probability distribution for upper vector]\n",
    "\n",
    "    Returns:\n",
    "        [float]: [Weighted average gini-value between two vectors.]\n",
    "    \"\"\"\n",
    "    return prob_low * giniValue(dist_low) + prob_high*giniValue(dist_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def probability_dist_label(y):\n",
    "    \"\"\"Calculate the probability distribution for a vector y (usually classification labels).\n",
    "\n",
    "    Args:\n",
    "        y ([1D Array]): [Vector ]\n",
    "\n",
    "    Returns:\n",
    "        [1D array]: [Probability for each value in the vector y]\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    n_tot = len(y)\n",
    "    \n",
    "    # Dict, with unique values in y as keys, and the values being the number of observed outcomes. \n",
    "    count_labels = Counter(y)\n",
    "\n",
    "    # Calculate probability by counting numbers of each class. \n",
    "    probs = []\n",
    "    for val in count_labels.values():\n",
    "        probs.append(val/n_tot)\n",
    "    return np.array(probs)\n",
    "\n",
    "def informationGain(X,y):\n",
    "   \"\"\"[Calculates the information gain IG for each feature in a feature matrix \"X\".]\n",
    "\n",
    "   Args:\n",
    "       X ([nD-array]): [Feature Matrix]\n",
    "\n",
    "   Returns:\n",
    "       [1D List]: [list of Information gain for each feature]\n",
    "   \"\"\"\n",
    "   # Store IG for each of the features in list. \n",
    "   IG_list = []\n",
    "\n",
    "   # Number of rows. \n",
    "   N_tot = len(X)\n",
    "\n",
    "   # Original Distributions before observations. (used for E(T) and G(T))\n",
    "   T = probability_dist_label(y)\n",
    "   \n",
    "   # Loop over each Feature-column. \n",
    "   n_col = len(X[0])\n",
    "   for i in range(n_col):\n",
    "      feature_i = X[:,i]\n",
    "      # Split feature based on its mean value. \n",
    "      index_low =  feature_i <  np.mean(feature_i)\n",
    "      index_high = feature_i >=  np.mean(feature_i)\n",
    "\n",
    "      # Split Feature and Labels. \n",
    "      feature_low = feature_i[index_low]\n",
    "      y_low = y[index_low]\n",
    "\n",
    "      feature_high = feature_i[index_high]\n",
    "      y_high = y[index_high]\n",
    "\n",
    "      # Probability for being below or above mean. \n",
    "      prob_low = len(feature_low) / N_tot\n",
    "      prob_high = len(feature_high) / N_tot\n",
    "\n",
    "      # Calculate Proability distributions of Labels in new categories (below and above mean.)\n",
    "      dist_low = probability_dist_label(y_low)\n",
    "      dist_high = probability_dist_label(y_high)\n",
    "      \n",
    "      # Calculate the IG based on its definition and append to list. \n",
    "      HA = H_A(prob_low, dist_low, prob_high, dist_high)\n",
    "      IG = H(T) - HA \n",
    "      IG_list.append(IG)\n",
    "      continue\n",
    "   return IG_list\n",
    "\n",
    "    \n",
    "def giniList(X,y):\n",
    "    \"\"\"Calculate the Gini-Index for every feature in X, by splitting the data on its mean value.\n",
    "\n",
    "    (Almost Identical Implementation as for Informationgain(X) function.)\n",
    "\n",
    "    Args:\n",
    "        X ([nD-Array]): [Feature Matrix]\n",
    "        y ([1D-Array]): [Labels/Classes - Target vector/True Cases]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    # Store gini values for every feature. \n",
    "    gini_values = []\n",
    "\n",
    "    # total Gini value before splitting. \n",
    "    T = probability_dist_label(y)\n",
    "    N_tot = len(y)\n",
    "   \n",
    "    # Loop over each Feature\n",
    "    n_col = len(X[0])\n",
    "    for i in range(n_col):\n",
    "        feature_i = X[:,i]\n",
    "    \n",
    "        # Find index of feature based on its mean value. \n",
    "        index_low =  feature_i <  np.mean(feature_i)\n",
    "        index_high = feature_i >=  np.mean(feature_i)\n",
    "       \n",
    "        # Split Feature and Labels on mean index. \n",
    "        feature_low = feature_i[index_low]\n",
    "        y_low = y[index_low]\n",
    "        \n",
    "        feature_high = feature_i[index_high]\n",
    "        y_high = y[index_high]\n",
    "\n",
    "        # Calculate Probability for being below or above mean. \n",
    "        prob_low = len(feature_low) / N_tot\n",
    "        prob_high = len(feature_high) / N_tot\n",
    "\n",
    "        # Generate probability distributions for labels.\n",
    "        dist_low = probability_dist_label(y_low)\n",
    "        dist_high = probability_dist_label(y_high)\n",
    "\n",
    "        # Calculate gini_gain before and after split. \n",
    "        gini_gain = giniValue(T) - G_A(prob_low, dist_low,prob_high,dist_high) \n",
    "        gini_values.append(gini_gain)\n",
    "        continue\n",
    "\n",
    "    return gini_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tree:\n",
    "    \"\"\"[A decision Tree Class used for Machine Learning classification.]\n",
    "\n",
    "    Returns:\n",
    "        [Object]: [An instance of this class, That can be trained to perform predicitons.]\n",
    "    \"\"\"\n",
    "    # Store Right/LEft Branches (called nodes)\n",
    "    nodes_left = None\n",
    "    nodes_right = None\n",
    "\n",
    "    # Store Feature Split Value. \n",
    "    root = None\n",
    "\n",
    "    # Store Leaf Value (No branches if this is different from None).  \n",
    "    leaf = None \n",
    "\n",
    "    def __init__(self, val = None, feat = None):\n",
    "        \"\"\"Nodes in the tree are defined by other tree instances. \n",
    "\n",
    "        Args:\n",
    "            val ([float], optional): [Feature split value (usually mean)]. Defaults to None for root.\n",
    "            feat ([int], optional): [column index for features]. Defaults to None - set during learning.\n",
    "        \"\"\"\n",
    "        # Store Feature and Splitting Value in Tree-Node. \n",
    "        self.root = val\n",
    "        self.feature = feat\n",
    "\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Create a copy of the current Tree-Object.\n",
    "\n",
    "        Returns:\n",
    "            [Tree()]: [Copy of self]\n",
    "        \"\"\"\n",
    "        copy_tree = tree()\n",
    "        copy_tree.feature = self.feature\n",
    "        copy_tree.leaf = self.leaf\n",
    "        copy_tree.nodes_right = self.nodes_right\n",
    "        copy_tree.nodes_left = self.nodes_left\n",
    "        copy_tree.root = self.root\n",
    "        return copy_tree\n",
    "\n",
    "    def print_leaf(self):\n",
    "        \"\"\"Itterate over all leafs, and prints them (used during testing).\n",
    "        \"\"\"\n",
    "        if self.leaf != None:\n",
    "            print(self.leaf)\n",
    "            \n",
    "        if self.nodes_right:\n",
    "            right_branch = self.nodes_right \n",
    "            right_branch.print_leaf()\n",
    "        if self.nodes_left:\n",
    "            left_branch = self.nodes_left \n",
    "            left_branch.print_leaf()\n",
    "        return\n",
    "\n",
    "    def get_leaf(self, leaf_dict):\n",
    "        \"\"\"Adds a leaf of current branch to input dictionary.\n",
    "\n",
    "        Args:\n",
    "            leaf_dict ([dict]): [dict of labels e.g {\"g\":102, \"h\": 123}]\n",
    "\n",
    "        Returns:\n",
    "            [dict]: [Returns dict with a leaf added the counted values.]\n",
    "        \"\"\"\n",
    "        if self.leaf != None:\n",
    "                if self.leaf not in leaf_dict:\n",
    "                    leaf_dict[self.leaf] = 1\n",
    "                    return leaf_dict\n",
    "                else:\n",
    "                    leaf_dict[self.leaf] += 1\n",
    "                    return leaf_dict\n",
    "        else:\n",
    "            return leaf_dict\n",
    "\n",
    "    def find_leaf(self, leaf_dict = {}):\n",
    "        \"\"\"Itterates over all leafs and return a dictionary with labels as keys and their count as values.\n",
    "\n",
    "        Args:\n",
    "            leaf_dict (dict, optional): [the dict to store values in, initilized as the empty dict.]. Defaults to {}.\n",
    "\n",
    "        Returns:\n",
    "            [dict]: [A dictionary where the type of leafs have been counted]\n",
    "        \"\"\"\n",
    "        # Check if nodes are not None. \n",
    "        if self.nodes_left: \n",
    "            leaf_dict = self.nodes_left.get_leaf(leaf_dict)\n",
    "            self.nodes_left.find_leaf(leaf_dict)\n",
    "        if self.nodes_right:\n",
    "            leaf_dict = self.nodes_right.get_leaf(leaf_dict)\n",
    "            self.nodes_right.find_leaf(leaf_dict)\n",
    "                \n",
    "        return leaf_dict\n",
    "\n",
    "\n",
    "    def count_correct_predictions(self, y_true,y_pred, print_results = True):\n",
    "        \"\"\"Count the number of correct and faulty predictions given a prediction-vector and true values. \n",
    "\n",
    "        Args:\n",
    "            y_true ([1D array]): [True values (usually test set)]\n",
    "            y_pred ([1D array]): [Predicted values]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [Returns the number of correct predictions and prints out relevant prediction ratios.]\n",
    "        \"\"\"\n",
    "\n",
    "        # Force numpy to convert Boolean data to binary. \n",
    "        binary_representation = np.array((y_pred == y_true), dtype = int)\n",
    "    \n",
    "        # Compute Correct, and False predictions. \n",
    "        N_samples = len(binary_representation)\n",
    "        N_correct = np.sum(binary_representation)\n",
    "        N_false = N_samples - N_correct\n",
    "\n",
    "        # Print Ratio statements about accuracy.\n",
    "        if print_results:\n",
    "            print(\"Correct Predictions\", N_correct, \"/\", N_samples )\n",
    "            print(\"Missed Predictions\", N_false, \"/\", N_samples )\n",
    "        return np.sum(binary_representation)\n",
    "\n",
    "    def r2_score(self, y_true, y_pred):\n",
    "        \"\"\"Compute R-Squared value given predicted and true values.\n",
    "\n",
    "        Args:\n",
    "            y_true ([1D array]): [True values]\n",
    "            y_pred ([1D array]): [Predicted values]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [R-Score, values close to 1 correspond to high accuracy.]\n",
    "        \"\"\"\n",
    "        import pandas as pd \n",
    "        \n",
    "        # Convert from catagorical predictions, to binary representation. \n",
    "        df = df = pd.DataFrame(data=({0:y_true, 1:y_pred}))\n",
    "        df_binary = pd.get_dummies(df)\n",
    "\n",
    "        y_pred_binary = df_binary.iloc[:,1]\n",
    "        y_true_binary = df_binary.iloc[:,0]\n",
    "\n",
    "        # Compute Definitions of R2.\n",
    "        SSr = np.sum((y_pred_binary- y_true_binary)**2)\n",
    "        SSt = np.sum((y_true_binary - np.mean(y_true_binary))**2)\n",
    "\n",
    "        # R-Score. \n",
    "        return 1 - SSr/SSt\n",
    "\n",
    "    def traverseTree(self,X):\n",
    "        \"\"\"Traverse down a \"learned-Tree\" based on values in X, and locate leafs for predictions.\n",
    "\n",
    "        Args:\n",
    "            X ([nD-Array]): [Feature Matrix to be predicted.]\n",
    "\n",
    "        Returns:\n",
    "            [Scalar']: [Predicted value for a given row in X]\n",
    "        \"\"\"\n",
    "        # If there is a leaf, we have located an prediction. \n",
    "        if self.leaf != None:\n",
    "                return self.leaf \n",
    "        # Otherwise Keeep traversing. \n",
    "        else:\n",
    "            if X[self.feature] < self.root:\n",
    "                node_low = self.nodes_left \n",
    "                return node_low.traverseTree(X)\n",
    "            else:\n",
    "                node_high = self.nodes_right \n",
    "                return node_high.traverseTree(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"[Predict each row in feature matrix X, by using previously defined traverse function on each row. ]\n",
    "\n",
    "        Args:\n",
    "            X ([nD-Array]): [Feature Matrix - X_test]\n",
    "            impurity_measure (str, optional): [Feature Selection Metric]. Defaults to 'entropy'.\n",
    "\n",
    "        Returns:\n",
    "            [1d Array]: [Predictions - Y_pred]\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the learn-function has been called. (Must be stored right/left nodes in root.)\n",
    "        if not self.nodes_right and not self.nodes_left:\n",
    "            print(\"Can Predict With An Un-Fitted Tree\")\n",
    "            return \n",
    "            \n",
    "        # Initilize Vector to store predictions in. \n",
    "        n_row = len(X)\n",
    "        pred_y = list(range(n_row))\n",
    "\n",
    "        # Traverse each row in feature matrix. \n",
    "        for i in range(n_row):\n",
    "            pred_y[i] = self.traverseTree(X[i,:])\n",
    "            \n",
    "        return np.array(pred_y, dtype=\"object\")\n",
    "\n",
    "\n",
    "    def prune_self(self, X_prune, y_prune):\n",
    "        \"\"\"Reduce the number of leafs/branches by pruning a trained tree.\n",
    "\n",
    "        Args:\n",
    "            X_prune ([nd-Matrix]): [Feature Matrix]\n",
    "            y_prune ([1d-vector]): [Target/label vector]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Original Predictions before pruning. \n",
    "        original_pred = self.predict(X_prune)\n",
    "        original_correct = self.count_correct_predictions(y_prune, original_pred, print_results=False)\n",
    "\n",
    "        # Prune Right Branch. \n",
    "        if self.nodes_right and not self.nodes_right.leaf:\n",
    "        \n",
    "            #Get Label/Leaf that occurs most from sub-tree.\n",
    "            sub_tree = self.copy().nodes_left\n",
    "            leaf_dict = sub_tree.find_leaf()\n",
    "            max_leaf = max(leaf_dict, key=leaf_dict.get)\n",
    "\n",
    "            # Exchange sub-branch with leaf. \n",
    "            prune_tree = self.copy()\n",
    "            prune_tree.nodes_left.leaf = max_leaf\n",
    "            prune_pred = prune_tree.predict(X_prune)\n",
    "            prune_correct = self.count_correct_predictions(y_prune, prune_pred ,print_results=False)\n",
    "\n",
    "            # Swap out left branch with most occuring leaf. \n",
    "            if prune_correct >= original_correct:\n",
    "                self.nodes_right.leaf = max_leaf\n",
    "                self.nodes_right.nodes_right = None\n",
    "                self.nodes_left.nodes_left  = None\n",
    "            else:\n",
    "                self.nodes_left.prune_self(X_prune,y_prune)\n",
    "\n",
    "        # Prune Right Branch. \n",
    "        if self.nodes_left and not self.nodes_left.leaf:\n",
    "            #Get Label/Leaf that occurs most from sub-tree. \n",
    "            sub_tree = self.copy().nodes_left\n",
    "            leaf_dict = sub_tree.find_leaf()\n",
    "            max_leaf = max(leaf_dict, key=leaf_dict.get)\n",
    "\n",
    "            # Exchange branch with leaf. \n",
    "            prune_tree = self.copy() \n",
    "            prune_tree.nodes_left.leaf = max_leaf\n",
    "            prune_pred = prune_tree.predict(X_prune)\n",
    "            prune_correct = self.count_correct_predictions(y_prune, prune_pred, print_results=False)\n",
    "\n",
    "            # Swap out left branch with most occuring leaf. \n",
    "            if prune_correct >= original_correct:\n",
    "                self.nodes_left.leaf = max_leaf\n",
    "                self.nodes_left.nodes_right = None\n",
    "                self.nodes_left.nodes_left  = None\n",
    "            else:\n",
    "                self.nodes_left.prune_self(X_prune,y_prune)\n",
    "        return\n",
    "\n",
    "    def learn_fullTree(self,X,y,impurity_measure='entropy'):\n",
    "        \"\"\"Learn or train the given tree class on a feature matrix X, with corresponding label-vector y. \n",
    "\n",
    "        Args:\n",
    "            X ([nD-Array]): [Feature Matrix - X_train]\n",
    "            y ([1d-Array]): [Target Vector - Y_train]\n",
    "\n",
    "        \"\"\"\n",
    "           \n",
    "        # Convert y to numpy array if list - otherwise assume that it is. \n",
    "        if not isinstance(y, np.ndarray): \n",
    "            y = np.array(y)\n",
    "        \n",
    "        # Convert X to numpy array. \n",
    "        if isinstance(X, list):\n",
    "            X = np.array(X)\n",
    "\n",
    "        # Y is the single scalar value.\n",
    "        if y.size == 0:\n",
    "            print(\"EMPTY!\")\n",
    "            \n",
    "        # All Labels Equal (Base Case). \n",
    "        if np.all(y == y[0]):\n",
    "            self.leaf = y[0]\n",
    "            return\n",
    "\n",
    "        # All rows are equal (--> Identical Features. )\n",
    "        # Return a leaf with the most common label (Base Case)\n",
    "        elif (X == X[0]).all():\n",
    "            countValues = Counter(y)\n",
    "            self.leaf = max(countValues, key=countValues.get)\n",
    "            return\n",
    "        else:\n",
    "            # Select next feature based MAX IG.  \n",
    "            if impurity_measure == 'entropy': \n",
    "                IG_list = informationGain(X,y)\n",
    "                max_feature_index = IG_list.index(max(IG_list))\n",
    "                feature = X[:,max_feature_index]\n",
    "            \n",
    "            # Select next feature based on MIN Gini. \n",
    "            elif impurity_measure == 'gini':\n",
    "                 gini_list = giniList(X,y)\n",
    "                 max_feature_index = gini_list.index(max(gini_list))\n",
    "                 feature = X[:,max_feature_index]\n",
    "                \n",
    "            # Split based on mean. \n",
    "            index_low = feature <  np.mean(feature)\n",
    "            index_high = feature >= np.mean(feature)\n",
    "\n",
    "            # Split X and Y vectors in Left/Right Branch. \n",
    "            X_low = X[index_low,:]\n",
    "            y_low= y[index_low]\n",
    "            \n",
    "            X_high = X[index_high,:]\n",
    "            y_high = y[index_high]\n",
    "            \n",
    "            # Set Splitting value to root of this branch-node for later traversal. \n",
    "            self.root = np.mean(feature)\n",
    "            self.feature = max_feature_index\n",
    "    \n",
    "            # Create new branches if there are data points for these vectors.\n",
    "            if y_low.size != 0:\n",
    "                left_branch = tree()\n",
    "                left_branch.learn_fullTree(X_low,y_low, impurity_measure = impurity_measure)\n",
    "                self.nodes_left = (left_branch)\n",
    "\n",
    "            # Learn and store right branch. \n",
    "            if y_high.size != 0:\n",
    "                right_branch = tree()\n",
    "                right_branch.learn_fullTree(X_high, y_high, impurity_measure = impurity_measure)\n",
    "                self.nodes_right = (right_branch)\n",
    "        return\n",
    "\n",
    "    def learn(self,X,y,impurity_measure='entropy', prune = False, prune_size = 0.15):\n",
    "        \"\"\"Main Learning function used in root-instance of this tree class when fitting training data. \n",
    "\n",
    "        Args:\n",
    "            X ([nd-Array (dtype = floats)]): [Feature Matrix]\n",
    "            y ([1d-Array (dtype = objects)]): [Labels / Classes]\n",
    "            impurity_measure (str, optional): [Information selection metric]. Defaults to 'entropy', \"gini\" is second option.\n",
    "            prune (bool, optional): [Prunes tree and reduces number of branches / leafs]. Defaults to False.\n",
    "            prune_size (float, optional): [test_size to split the trainin data in, if prune.]. Defaults to 0.15.\n",
    "        \"\"\"\n",
    "        # Perform Normal Learning Then Prune the tree. \n",
    "        if prune:\n",
    "            X2, X_prune, y2, y_prune = train_test_split(X,y, test_size = prune_size)\n",
    "            # First Build Initial Tree, Then Prune it. \n",
    "            self.learn_fullTree(X2,y2, impurity_measure)\n",
    "            self.prune_self(X_prune, y_prune)\n",
    "        else:\n",
    "             self.learn_fullTree(X,y, impurity_measure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATA FOR TESTING. \n",
    "magic_data  = pd.read_csv(\"magic04.data\", header = None)\n",
    "\n",
    "# NOTE- This was mainly used during test-phase. And in the pdf report N_samples = 15000 were used when documenting some results.  \n",
    "N_samples = len(magic_data)\n",
    "\n",
    "# Sample data for quicker learning and predictions \n",
    "magic_sample = magic_data.sample(n = N_samples)\n",
    "\n",
    "# Convert data to numpy arrays. \n",
    "X = magic_sample.iloc[:,0:9].to_numpy()\n",
    "y = magic_sample[10].to_numpy()\n",
    "\n",
    "# Test and Training Sets (For when building the algorithm). \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size =0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BEFORE PRUNING: Prediction and Leafs.\n",
      "leafs {'g': 1532, 'h': 1195}\n",
      "Correct Predictions 4502 / 5706\n",
      "Missed Predictions 1204 / 5706\n",
      "\n",
      " AFTER PRUNING: Prediction and Leafs.\n",
      "Leafs {'g': 964, 'h': 790}\n",
      "Correct Predictions 4460 / 5706\n",
      "Missed Predictions 1246 / 5706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4460"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the number of leafs before and after pruning, using the self.find_leaf() function. \n",
    "# Not Pruned. \n",
    "print(\"\\n BEFORE PRUNING: Prediction and Leafs.\")\n",
    "my_tree = tree()\n",
    "impurity_measure = \"entropy\"\n",
    "my_tree.learn(X_train,Y_train, impurity_measure = impurity_measure, prune = False)\n",
    "print(\"leafs\", my_tree.find_leaf(leaf_dict={}))\n",
    "y_pred = my_tree.predict(X_test)\n",
    "my_tree.count_correct_predictions(Y_test, y_pred)\n",
    "\n",
    "# Pruned \n",
    "print(\"\\n AFTER PRUNING: Prediction and Leafs.\")\n",
    "my_tree = tree()\n",
    "impurity_measure = \"entropy\"\n",
    "my_tree.learn(X_train,Y_train, impurity_measure = impurity_measure, prune = True, prune_size = 0.15)\n",
    "print(\"Leafs\", my_tree.find_leaf(leaf_dict={}))\n",
    "y_pred = my_tree.predict(X_test)\n",
    "my_tree.count_correct_predictions(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned Tree Accuracy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.781633368384157"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to show the accuracy of the pruned tree.  \n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = my_tree.predict(X_test)\n",
    "print(\"Pruned Tree Accuracy.\")\n",
    "accuracy_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4) Evaluate your algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 70/30 Traning/Testing. \n",
    "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X,y, test_size =0.3)\n",
    "\n",
    "# Split into 50/50 Validation/Testing data. \n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test,Y_val_test, test_size =0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Validation Scores \n",
      "\n",
      "Model:  entropy| No Prune -Score =  0.7960042060988434\n",
      "Model:  entropy| No Prune -Score =  0.7960042060988434\n",
      "Model:  entropy| No Prune -Score =  0.7960042060988434\n",
      "Model:  entropy| prune-size =0.05 -Score =  0.7921486154924641\n",
      "Model:  entropy| prune-size =0.15 -Score =  0.7872415001752541\n",
      "Model:  entropy| prune-size =0.75 -Score =  0.650192779530319\n",
      "Model:  gini| No Prune -Score =  0.7956536978618998\n",
      "Model:  gini| No Prune -Score =  0.7956536978618998\n",
      "Model:  gini| No Prune -Score =  0.7956536978618998\n",
      "Model:  gini| prune-size =0.05 -Score =  0.7942516649141255\n",
      "Model:  gini| prune-size =0.15 -Score =  0.7907465825446898\n",
      "Model:  gini| prune-size =0.75 -Score =  0.650192779530319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Parameter Space. (9 Models in total.)\n",
    "impurity_metric = [\"entropy\", \"gini\"]\n",
    "prune_metric = [False, True]\n",
    "prune_sizes = [0.05,0.15,0.75]\n",
    "\n",
    "# Store Parameter specifc names and corresponding accuracy scores. \n",
    "model_name = []\n",
    "model_accuracy = []\n",
    "model_list = []\n",
    "\n",
    "\n",
    "# Loop over models, Train and evaluate accuracy. \n",
    "for infoormation_metric in impurity_metric:\n",
    "    for prune_val in prune_metric:\n",
    "        for prune_size in prune_sizes:\n",
    "            \n",
    "            if prune_val:\n",
    "                model = tree()\n",
    "                model.learn(X_train, Y_train, impurity_measure=infoormation_metric, prune=prune_val, prune_size=prune_size)\n",
    "                name = infoormation_metric + \"| prune-size =\" + str(prune_size)\n",
    "                model_name.append(name)\n",
    "            else:\n",
    "                model = tree()\n",
    "                model.learn(X_train, Y_train, impurity_measure=infoormation_metric)\n",
    "                name = infoormation_metric + \"| No Prune\"\n",
    "                model_name.append(name)\n",
    "\n",
    "            Y_val_predicted = model.predict(X_val)\n",
    "            accuracy = accuracy_score(Y_val, Y_val_predicted)\n",
    "            model_accuracy.append(accuracy)\n",
    "            model_list.append(model)\n",
    "\n",
    "\n",
    "print(\"Model Validation Scores \\n\")\n",
    "for i, name in enumerate(model_name):\n",
    "    print(\"Model: \",name, \"-Score = \", model_accuracy[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikf\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEvCAYAAABhSUTPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWLklEQVR4nO3df5BdZ33f8fenK8yPQPhRb0hHsi3FiDiiwa69FSUhY7fEjQwJgsQZyxCIKYlqJgKSiSnOTEMoDAyuQ2ESi6gKozruMIgQCCipEkE9xZAQasnUv2QQs5Ep3jgdr3GCsUsRMt/+cY+c6+vV7pGeu9a19X7N7Ox9nvPdc76r0Z757HPOnpuqQpIkScfnH53oBiRJkh7PDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNVpyoA5966qm1evXqE3V4SZKk3m666aZ7q2p6oW0nLEytXr2affv2najDS5Ik9Zbkfx9tm5f5JEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGvQKU0k2JDmQZDbJlQtsf2aSP0lyS5L9SV4//lYlSZImz5JhKskUsBW4CFgHXJpk3UjZrwB3VNXZwAXA+5KcMuZeJUmSJk6flan1wGxVHayqQ8BOYONITQHPSBLg6cB9wOGxdipJkjSB+oSplcBdQ+O5bm7YNcCPAHcDtwFvqarvje4oyeYk+5Lsm5+fP86WJUmSJkeft5PJAnM1Mv4p4GbgXwFnAp9J8vmquv8RX1S1HdgOMDMzM7qPZXXeW697LA8nqXPT1a870S1I0rLqE6bmgNOGxqsYrEANez3w3qoqYDbJncBZwI1j6VKSJtTX3/mjJ7oF6aR0+ttvO9EtPKzPZb69wNoka7qbyjcBu0Zqvg68FCDJc4EfBg6Os1FJkqRJtOTKVFUdTrIF2ANMATuqan+Sy7vt24B3AdcmuY3BZcG3VdW9y9i3JEnSROhzmY+q2g3sHpnbNvT6buBfj7c1SZKkyecT0CVJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhr0ClNJNiQ5kGQ2yZULbH9rkpu7j9uTPJTkOeNvV5IkabIsGaaSTAFbgYuAdcClSdYN11TV1VV1TlWdA/wGcENV3bcM/UqSJE2UPitT64HZqjpYVYeAncDGReovBT4yjuYkSZImXZ8wtRK4a2g81809SpKnARuAj7e3JkmSNPn6hKksMFdHqf0Z4C+PdokvyeYk+5Lsm5+f79ujJEnSxOoTpuaA04bGq4C7j1K7iUUu8VXV9qqaqaqZ6enp/l1KkiRNqD5hai+wNsmaJKcwCEy7RouSPBM4H/jUeFuUJEmaXCuWKqiqw0m2AHuAKWBHVe1Pcnm3fVtX+irg01X14LJ1K0mSNGGWDFMAVbUb2D0yt21kfC1w7bgakyRJejzwCeiSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNeoWpJBuSHEgym+TKo9RckOTmJPuT3DDeNiVJkibTiqUKkkwBW4ELgTlgb5JdVXXHUM2zgA8CG6rq60l+YJn6lSRJmih9VqbWA7NVdbCqDgE7gY0jNa8GPlFVXweoqnvG26YkSdJk6hOmVgJ3DY3nurlhzweeneSzSW5K8rqFdpRkc5J9SfbNz88fX8eSJEkTpE+YygJzNTJeAZwHvBz4KeA3kzz/UV9Utb2qZqpqZnp6+piblSRJmjRL3jPFYCXqtKHxKuDuBWruraoHgQeTfA44G/jqWLqUJEmaUH1WpvYCa5OsSXIKsAnYNVLzKeAnkqxI8jTgRcCXx9uqJEnS5FlyZaqqDifZAuwBpoAdVbU/yeXd9m1V9eUkfw7cCnwP+FBV3b6cjUuSJE2CPpf5qKrdwO6RuW0j46uBq8fXmiRJ0uTzCeiSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNeoWpJBuSHEgym+TKBbZfkOSbSW7uPt4+/lYlSZImz4qlCpJMAVuBC4E5YG+SXVV1x0jp56vqp5ehR0mSpInVZ2VqPTBbVQer6hCwE9i4vG1JkiQ9PvQJUyuBu4bGc93cqBcnuSXJnyV5wVi6kyRJmnBLXuYDssBcjYy/BJxRVQ8keRnwSWDto3aUbAY2A5x++unH1qkkSdIE6rMyNQecNjReBdw9XFBV91fVA93r3cCTkpw6uqOq2l5VM1U1Mz093dC2JEnSZOgTpvYCa5OsSXIKsAnYNVyQ5AeTpHu9vtvvN8bdrCRJ0qRZ8jJfVR1OsgXYA0wBO6pqf5LLu+3bgIuBNyY5DHwb2FRVo5cCJUmSnnD63DN15NLd7pG5bUOvrwGuGW9rkiRJk88noEuSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDUwTEmSJDXoFaaSbEhyIMlskisXqfvnSR5KcvH4WpQkSZpcS4apJFPAVuAiYB1waZJ1R6m7Ctgz7iYlSZImVZ+VqfXAbFUdrKpDwE5g4wJ1bwI+Dtwzxv4kSZImWp8wtRK4a2g81809LMlK4FXAtsV2lGRzkn1J9s3Pzx9rr5IkSROnT5jKAnM1Mv4A8LaqemixHVXV9qqaqaqZ6enpni1KkiRNrhU9auaA04bGq4C7R2pmgJ1JAE4FXpbkcFV9chxNSpIkTao+YWovsDbJGuBvgE3Aq4cLqmrNkddJrgX+1CAlSZJOBkuGqao6nGQLg7/SmwJ2VNX+JJd32xe9T0qSJOmJrM/KFFW1G9g9MrdgiKqqy9rbkiRJenzwCeiSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNDFOSJEkNeoWpJBuSHEgym+TKBbZvTHJrkpuT7EvykvG3KkmSNHlWLFWQZArYClwIzAF7k+yqqjuGyq4HdlVVJXkh8IfAWcvRsCRJ0iTpszK1HpitqoNVdQjYCWwcLqiqB6qquuH3AYUkSdJJoE+YWgncNTSe6+YeIcmrknwF+G/AvxlPe5IkSZOtT5jKAnOPWnmqqj+uqrOAVwLvWnBHyebunqp98/Pzx9SoJEnSJOoTpuaA04bGq4C7j1ZcVZ8Dzkxy6gLbtlfVTFXNTE9PH3OzkiRJk6ZPmNoLrE2yJskpwCZg13BBkuclSff6XOAU4BvjblaSJGnSLPnXfFV1OMkWYA8wBeyoqv1JLu+2bwN+Dnhdku8C3wYuGbohXZIk6QlryTAFUFW7gd0jc9uGXl8FXDXe1iRJkiafT0CXJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElqYJiSJElq0CtMJdmQ5ECS2SRXLrD9NUlu7T6+kOTs8bcqSZI0eZYMU0mmgK3ARcA64NIk60bK7gTOr6oXAu8Cto+7UUmSpEnUZ2VqPTBbVQer6hCwE9g4XFBVX6iqv+uGXwRWjbdNSZKkydQnTK0E7hoaz3VzR/MG4M8W2pBkc5J9SfbNz8/371KSJGlC9QlTWWCuFixM/iWDMPW2hbZX1faqmqmqmenp6f5dSpIkTagVPWrmgNOGxquAu0eLkrwQ+BBwUVV9YzztSZIkTbY+K1N7gbVJ1iQ5BdgE7BouSHI68AngtVX11fG3KUmSNJmWXJmqqsNJtgB7gClgR1XtT3J5t30b8HbgHwMfTAJwuKpmlq9tSZKkydDnMh9VtRvYPTK3bej1LwG/NN7WJEmSJp9PQJckSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWpgmJIkSWrQK0wl2ZDkQJLZJFcusP2sJH+V5DtJrhh/m5IkSZNpxVIFSaaArcCFwBywN8muqrpjqOw+4M3AK5ejSUmSpEnVZ2VqPTBbVQer6hCwE9g4XFBV91TVXuC7y9CjJEnSxOoTplYCdw2N57o5SZKkk16fMJUF5up4DpZkc5J9SfbNz88fzy4kSZImSp8wNQecNjReBdx9PAerqu1VNVNVM9PT08ezC0mSpInSJ0ztBdYmWZPkFGATsGt525IkSXp8WPKv+arqcJItwB5gCthRVfuTXN5t35bkB4F9wPcD30vyq8C6qrp/+VqXJEk68ZYMUwBVtRvYPTK3bej1/2Fw+U+SJOmk4hPQJUmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGhimJEmSGvQKU0k2JDmQZDbJlQtsT5Lf6bbfmuTc8bcqSZI0eZYMU0mmgK3ARcA64NIk60bKLgLWdh+bgd8bc5+SJEkTqc/K1HpgtqoOVtUhYCewcaRmI3BdDXwReFaSfzLmXiVJkibOih41K4G7hsZzwIt61KwE/na4KMlmBitXAA8kOXBM3epkdipw74luQscuv/2LJ7oFaTGeWx6vfiuP9RHPONqGPmFqoW7rOGqoqu3A9h7HlB4hyb6qmjnRfUh6YvHconHoc5lvDjhtaLwKuPs4aiRJkp5w+oSpvcDaJGuSnAJsAnaN1OwCXtf9Vd+/AL5ZVX87uiNJkqQnmiUv81XV4SRbgD3AFLCjqvYnubzbvg3YDbwMmAX+L/D65WtZJykvD0taDp5b1CxVj7q1SZIkST35BHRJkqQGhilJkqQGhilJkqQGhin1kuSVC7yN0HIc57Ik71hgfnWSSvKmoblrklx2DPt+R5K/SXJzktuTvGI8XUsahyTvTPKTS9S84sh7xCa5IMm1R6mrJO8bGl+x0LllkeNclmS+O1/ckeSX+36tTj6GKfX1SgbvzfgoSfo8/HUc7gHe0j2i43i9v6rOAX4e2JHkET8Dj+H3ImlEVb29qv77EjW7quq9PXb3HeBnk5za0NJHu/PFBcB7kjx3eKPnCx1hmDpJJfmFJDd2v3X95+4NrUnyQJJ3J7klyReTPDfJjwGvAK7u6s9M8tkk70lyA4OA89Ik/yvJbUl2JHlyt7+vJbmqO9aNSZ6X5BlJ7kzypK7m+7u6Jy3R9jxwPfCo9ydJck7X761J/jjJsxfbUVV9GTgMnLrA93JtkouH9v1A9/mCrvaPknwlyYeTpNt2XpIbktyUZI/vTSkdXZLf7H6GPpPkI0mu6OYf/tnrzgn/IcmXuvPKWd38ZUmu6XGYwwwee/BrCxz/jCTXd+eL65OcvtiOquoe4K+BM7oe/1OS/wFc1a14XzG079u7lfTVSb6c5PeT7E/y6SRP7WrOTPLn3fni80e+Nz1+GaZOQkl+BLgE+PHut66HgNd0m78P+GJVnQ18DvjlqvoCgwezvrWqzqmqv+5qn1VV5wNbgWuBS6rqRxk8v+yNQ4e8v6rWA9cAH6iqbwGfBV7ebd8EfLyqvtuj/fcCv34k/A25DnhbVb0QuA34rSX+DV4EfI9BQHv4e6mq9y3yZQD/DPhVBqt0PwT8eBcCfxe4uKrOA3YA7+7xvUgnnSQzwM8x+Fn6WWCxt3K5t6rOBX4PuGKRuqPZCrwmyTNH5q8BruvOFx8GfmeJnn+Iwc/7bDf1fOAnq+rXlzj+WmBrVb0A+HsG3zcMQt6buvPFFcAH+307mlQuUZ6cXgqcB+ztFlaeyuASGsAh4E+71zcBFy6yn492n38YuLOqvtqN/wD4FeAD3fgjQ5/f373+EPDvgE8yeMhrr/sRqurOJDcCrz4y150on1VVNwwd/2NH2cWvJfkF4FsMwl91/wYfPUr9qBuraq477s3AagYnyX8KfKbb1xQjb/It6WEvAT5VVd8GSPIni9R+ovt8E4PgdUyq6v4k1wFvBr49tOnFQ/v7r8B/PMouLknyEgaXDP9tVd3X/Yx/rKoe6tHCnVV1c/f6JmB1kqcDPwZ8rNsXwJN7fkuaUIapk1OAP6iq31hg23frH57k+hCL/x95cGh/i6nR11X1l90y+PnAVFXd3qPvI94D/BGDlbNj9f6q+u0F5h8cen2YbtW2u4w3fI/Wd4ZeH/n3CbC/ql58HP1IJ5ulzhfDjvy8LXUuWswHgC8B/2WRmqM9vfqjVbVlgfkFzxedpwy9Hj1fPLWr/fvuqoCeILzMd3K6Hrg4yQ8AJHlOkjOW+JpvAc84yravMPiN63nd+LXADUPbLxn6/FdD89cxWK1a7CT3KFX1FeAO4Ke78TeBv0vyE0c5/rH6GoOVO4CNwFL3ch0AppO8GCDJk5K8oOH40hPZXwA/k+Qp3SrNy5f6ghZVdR/wh8Abhqa/wOD2Ahjc4vAXDYf4GnAuQJJzgTVL9HM/cGeSn+++JknObji+JoBh6iRUVXcA/x74dJJbgc8AS90wvRN4a3eT+Zkj+/t/DC7VfSzJbQzuRdo2VPLkJP8TeAuPvBn0w8Cz+YfLgMfi3cCqofEvMrhB/lbgHOCdx7HPI34fOL+7nPgiHvlb6KNU1SHgYgY3o94C3MxgGV/SiKray+AezFsYXMbbB3xzmQ/7PmD4r/reDLy+O1+8lsG56Xh9HHhOd9n/jcBXFy8HBgHuDd35Yj+DX9r0OOZ782lZJfkaMFNV9y6w7WJgY1W9dmjuMmB1Vb3jsepR0mMrydOr6oEkT2NwuX5zVX3pOPZzAXBZVV023g6lY+M9UzohkvwucBHwshPdi6TH3PYMHgL8FAb3bx5zkJImiStTmihJzmHwl3mfPcGtSJpwSVYD51TVJ09wKzrJGaYkSZIaeAO6JElSA8OUJElSA8OUJElSA8OUJElSg/8PJgVJmzpuV48AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate Bar-Chart ()\n",
    "import seaborn as sns\n",
    "fig,ax  = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# For returning the 4 best models in the score-list.  \n",
    "import heapq\n",
    "best_4_vals = heapq.nlargest(4, model_accuracy)\n",
    "\n",
    "# Get the Best 4 Models. \n",
    "best_4_index = []\n",
    "for val in best_4_vals:\n",
    "    val_index =  model_accuracy.index(val)\n",
    "    best_4_index.append(val_index)\n",
    "    \n",
    "best_4_names = [model_name[i] for i in best_4_index]\n",
    "\n",
    "# If the numbers are close to identical, there might be plotted less than 4 bars. \n",
    "ax_bar = sns.barplot(best_4_names, best_4_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model:  = entropy| No Prune\n",
      "Best Model TestSet-Score =  0.8047669120224326\n",
      "Best Model Validation-Score =  0.7960042060988434\n"
     ]
    }
   ],
   "source": [
    "# Find the best model from accuracy scores. \n",
    "best_model_index = model_accuracy.index(max(model_accuracy))\n",
    "best_model = model_list[best_model_index]\n",
    "best_model_name = model_name[best_model_index]\n",
    "\n",
    "print(\"Best Model:  = \" + best_model_name)\n",
    "\n",
    "# Perform predictions on test-set. \n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "best_model_score = accuracy_score(Y_test, y_pred_best)\n",
    "\n",
    "print(\"Best Model TestSet-Score = \", best_model_score)\n",
    "print(\"Best Model Validation-Score = \", max(model_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix, Model =  entropy| No Prune\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate best results in Confusion Matrix. \n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusionMatrix = confusion_matrix(Y_test,y_pred_best)\n",
    "\n",
    "# Generate a Heat-Map Display of the confusion matrix. (From Class-Template Notebook.) \n",
    "sns.heatmap(data=confusionMatrix.round(2), annot=True, fmt='d', cmap=sns.color_palette(\"RdBu_r\", 1000))\n",
    "print(\"Confusion Matrix, Model = \", best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|GINI|\n",
      "SciKit-Learn-Gini Validation Score =  0.8250963897651595\n",
      "SciKit-Learn-Gini Test Score =  0.830354013319313\n",
      "\n",
      "|ENTROPY|\n",
      "SciKit-Learn-Entropy Validation Score =  0.8212407991587802\n",
      "SciKit-Learn-Entropy Test Score =  0.8131791097090781\n",
      "\n",
      "|COMMENT|\n",
      "Test score for Sklearn was found to be  0.830354013319313 VS my algorithm, with a test score of  0.8047669120224326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test-Set Score Comparison With Sk-Learn Algorithms.')"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAE/CAYAAABin0ZUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjB0lEQVR4nO3debxdVX338c/XBFQUR+JEUFBwwIlKCo4PWGsFqwYrVpyxKsUWrXWo2FpLtVqt2joQmqIP4ozWMdUo9tGCA1gSLKKgaERqYkQDDggiGPg9f+x1YXO4w0n2DTkJn/frdV7Ze6911v6dc/bN+d619zk3VYUkSZI2z022dgGSJEnbMsOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYknSDSvL0JJ/f2nXMJMkjkpw3S/vuSSrJwnnY17yNNcnaY9xzC4392STPnqX9xCT/sCX2LU0xTGleJbm0d7s6yeW99advxninJHneHH0WJ/lYkouS/DLJN5McPo/jPzfJd5L8KslPknwmyc6b8DC2iCRPS7K6Pbc/bm8qD9/adc2lqj5QVX9wQ+0vySuTrBzZ9r0Zth1WVV+uqnv1tl+Q5PcH7H+zj89pxprzeN1akuzRfuaPuyH3W1UHV9V7Wg2HJ/nKDbl/CQxTmmdVdcupG/BD4PG9bR/YQrt9H7AWuBtwe+BZwE/mY+AkBwCvB55aVTsD9wE+Mh9j9/axybMSSV4CvLXVdkfgrsBxwNL5rG2+baUZmC8BD0uyoNVwJ2AH4EEj2/ZsfefbFjs+t4QBr9GzgJ8DhyW56TyWNK10fA/TZKgqb962yA24APj9tnwT4Gjg+8DFdIHkdq3tZsD72/ZfAKvoAsLrgKuA3wCXAsfOsJ9LgX1mqePBwGlt7G8AB7btc44PvAz45Cxj3xx4C/C/wC+BrwA3b21PAM5p+z0FuM/Ic/MK4GzgCmDhTHVOs89bt3qfPEtdN6ULW+vb7a3ATVvbgcA64K+AnwI/Bg4BHgt8F/gZ8Ne9sY4BPgp8GPgV8HXggb32qdf1V8C5wBN7bYcDXwX+pY37D23bV1p7WttP2/N3NnC/3uN8L7ChPb+vAm7SG/crwJvp3sB/ABw8w3OxI/BrYN+2/sfAu4FTR7at6T8/bfl9wNXA5e05/ytgd6CAZ9P9wnAR8DezvBYzHp+9sRa29Se1Y+N+M/Q/BXjeDG1/Any7PR8nA3frtb2NLtBdApwJPGKa1/f9rf15bT+vba/dr4DPA7vM8fP+feAFdEHx0JG2AvZsy7cH/qPta1U7Jr7S6/vQtv2X7d+Hjjz+17W6LqcLwKe0mu9D97N8VXvOf9HucyKwDPhMeyz/DdxjpLY/A77X2l8L3AM4vdX4EWDH1ncX4NN0P6M/A75MOya93bhvW70Ab9vvjeuGqRcDXwMW073R/xvwodb2p+0/152ABcC+wK1a24xvHr39/L/2n+thwF1H2nalC2mPpQt0j27ri8YZH3hE+0/774GH0QJJr31ZG2PXVvtD2+O7J3BZ298OdG/Ca3r/KV8AnAXsRhfIZq1zZJ8HARtpb8Az1P2a9nzfAVhEF9Je29oObPd/davt+XSB5YPAzsB925vS3Vv/Y4DfAoe2/i+jCy87tPYnA3dpdT+lPe47t7bD275eSBcYb851w9Rj6N7cb0MXrO7Tu+97gU+1mnanC3rP7Y3721b7Aro38fVAZng+/gv4y7Z8LF3weN3IthN6z8+66Y7jtr473RvwO9vjeSBdIL7PDPue7ficGmsh8By6Y2TPWV7XU5jmeKULw2va87eQLnie1mt/Bl2IWQi8FLgQuNnI63tIew1v3vbzfbrjeGr9DXP8nFwB3BZ4B7BipL0fpk5qt52AvelC3tTxcDu6MPjMVutT2/rte4//h3TH6EK64/Ga54TesdXb94l0wWe/dp8PACeN1LYCuFUb9wrgC8Dd6QL9ucCzW99/BJa3/e7QHve0x5y3G9dtqxfgbfu9cd0w9W3gUb22O7f/wBfSvbGdBjxgmjGmffMY6XNb4A10s0BX0YWU321trwDeN9L/5N5/juOMfzBd2PsF3W+8/0z3Bn4TuqD1wGnu87fAR3rrNwF+xLWzYhcAf9Jrn7XOke1PBy6co+bvA4/trT8GuKAtH9jqXtDWd25vKPv3+p8JHNKWjwG+NvJYfkxvdmNk32cBS9vy4cAPR9oP59o3z9+jC0kPpvcbfnt+rwD27m37U+CU3hhrem07tcdwpxlqOgb4RFv+BrAXXSjtb5s6Jg5kvDC1uLftDOCwzTg+p8Z6Gd2b9uLpxpjr5wH4LC1o9l6jX9ObnRrp/3Pacduemy9Ns59X9db/DPjcLHW9izaDCzyE7mf7Dr32optFWtDa7tVru2Zmii5EnTEy9unA4b26XjPTc8LMYepdvfXHAt8Zqe1hI8f+K3rrbwHe2pZfQxfwZwy83m6cN88364ZyN+ATSX6R5Bd04eoqutN576MLDiclWZ/kn5LsMN0gSf66d0H7coCq+nlVHV1V923jnQV8Mknafp88td+274fThbmxVNVnq+rxdL81L6X7D/t5dFP+N6MLLqPuQndqamqMq+l+A9+112ftyPMzbp0XA7vMcW3Ldfbflu/SH6OqrmrLl7d/+9fxXA7ccrpa22NZNzVekmclOatX9/3onpvpHud1VNUX6WaFlgE/SXJ8klu1++84zWPoP38X9sb5dVvs19z3JeDhSW5LN9v3PboA/9C27X5s+vVSF/aWfz3Tvuc4Pqe8HFhWVeumNiRZ3jvW/3qOWu4GvK33GvyMbqZv1zbWS5N8u10A/wu6GZe5XqOxHl+Sm9PNTn6gPd7T6WaPnjZN90V0v0D199dfHj1u4fqv+4zH0yzmeiyjx/5MPwtvopsB/HyS85McvRm1aDtkmNINZS3dNS236d1uVlU/qqrfVtXfV9XedKfJHkd3MSt0vzVeo6peX9de0H7k6E6q6iK662juQhd+1tLN+PT3e4uqesN048+mqq6uqi8AX6R7872I7nTYPabpvp7uDQ7oLpalO6X3o/6QveW56uw7ve33kFnKvc7+6S5QXz9L/7nsNrXQLvpdDKxPcje6011H0Z2KuQ3wLbo38imzPsdV9faq2pfuFMs96YLFRXQzGKOP4UfXH2Esp9MFiCPoTrlRVZfQPSdHAOur6gczlbiZ+7z+QNc/Pqf8AfCqJE/q9T2yd6y/fo6h1wJ/OnL83LyqTkvyCLqZzz8Gbtteo1+yCa/RHJ5Id4rsuCQXJrmQLvw8a5q+G+hO+y7ubduttzx63ML1X/fZap2312rawat+VVUvraq7A48HXpLkUVtyn9o2GKZ0Q1kOvK69+ZJkUZKlbfmRSe7fPll1Cd2b6NSsyU/orl2YUZI3JrlfkoXtKwteQHcK6GK6i2ofn+QxSRYkuVmSA5NM/Wc+6/hJliY5LMlt26eH9gMOoDvtdTVwAvDPSe7Sxn9I+yTTR4A/TPKoNsv2UrrTVqfNsKu56rxGVf2S7nqnZUkOSbJTkh2SHJzkn1q3D9G9OS9Kskvr//7Znsc57Jvkj9ps2IvbY/kacAu6N7AN7fl6Dl3QHEuS302yf3uOLqNdQNxmzT5Cd8zs3I6bl2zuY6iqy4HVbYwv95q+0rbNNis15zE4mzmOzynn0J12XJbkCXMMubAdH1O3Heh+vl6Z5L5tn7dO8uTWf2e6ALOh3ffVdOFnvjyb7ufg/sA+7fYwYJ8k9+93bK/rx4Fj2nF7b64bulYC90z3tR8LkzyF7rqqT49Zy0+AxUl2HPB4ZpTkcUn2bL8cXUL3/9RVc9xNNwKGKd1Q3kZ3kefnk/yK7o14/9Z2J7pPE11Cd/rvVK5903wbcGiSnyd5+wxj7wR8gu6apvPpfrN9AkBVraU7NffXdG8ma+lmPqaO/bnG/zndRc7fa/W9H3hTXfs1Dy8Dvkn3qaOfAW+ku/bnPLqLft9BN8vyeLqvibhyugcwRp2j/f+ZLgS8qtf/KOCTrcs/0IWHs1t9X2/bNten6C4un7o4+I/ajOK5dNeUnE73RnZ/2szPmG5FN7P1c7rTORfTzdxAd9H6ZXSv6VfoLpA/YcBjOJXugvz+9xB9uW2bLUz9I10w/UWSl23Gfmc8Pvuq6ht0s7LvTHLwLOP9K92pp6nbu6vqE3TH3klJLqGbHZwa42S6a6q+S/cc/4bNO1V2PUl2BR5Fd03Rhb3bmcDn6ILWqKPoZgkvpDvF/yG6cE4LmI+j++XjYroPbjyuzeiN44t0wfTCJOPeZ1PsRfeBgkvpjvnjquoUuObLQ+c6HavtVKq26KyopG1ckmPoLrh9xtauRdufJG+k++DAdMFL2iY4MyVJusEkuXeSB/ROmz+XbuZO2mZt138PSpI0cXamO7V3F7ova30L3WlkaZvlaT5JkqQBPM0nSZI0gGFKkiRpgK12zdQuu+xSu++++9bavSRJ0tjOPPPMi6pq0XRtWy1M7b777qxevXpr7V6SJGlsSUb/1NE1PM0nSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYkiRJGsAwJUmSNIBhSpIkaYCt9udk5tO+L3/v1i5B25kz3/SsrV2CJGkb4cyUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDbBdfJpPujH44Wvuv7VL0Hbmrq/+5tYuQdoujDUzleSgJOclWZPk6Gnab53kP5J8I8k5SZ4z/6VKkiRNnjlnppIsAJYBjwbWAauSrKiqc3vd/hw4t6oen2QRcF6SD1TVlVukaknSdulh73jY1i5B25mvvvCrW3wf48xM7QesqarzWzg6CVg60qeAnZMEuCXwM2DjvFYqSZI0gcYJU7sCa3vr69q2vmOB+wDrgW8Cf1FVV48OlOSIJKuTrN6wYcNmlixJkjQ5xglTmWZbjaw/BjgLuAuwD3Bskltd705Vx1fVkqpasmjRok0sVZIkafKME6bWAbv11hfTzUD1PQf4eHXWAD8A7j0/JUqSJE2uccLUKmCvJHsk2RE4DFgx0ueHwKMAktwRuBdw/nwWKkmSNInm/DRfVW1MchRwMrAAOKGqzklyZGtfDrwWODHJN+lOC76iqi7agnVLkiRNhLG+tLOqVgIrR7Yt7y2vB/5gfkuTJEmafP45GUmSpAEMU5IkSQMYpiRJkgYwTEmSJA1gmJIkSRrAMCVJkjSAYUqSJGkAw5QkSdIAhilJkqQBDFOSJEkDGKYkSZIGMExJkiQNYJiSJEkawDAlSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYkiRJGsAwJUmSNMBYYSrJQUnOS7ImydHTtL88yVnt9q0kVyW53fyXK0mSNFnmDFNJFgDLgIOBvYGnJtm736eq3lRV+1TVPsArgVOr6mdboF5JkqSJMs7M1H7Amqo6v6quBE4Cls7S/6nAh+ajOEmSpEk3TpjaFVjbW1/Xtl1Pkp2Ag4CPzdB+RJLVSVZv2LBhU2uVJEmaOOOEqUyzrWbo+3jgqzOd4quq46tqSVUtWbRo0bg1SpIkTaxxwtQ6YLfe+mJg/Qx9D8NTfJIk6UZknDC1CtgryR5JdqQLTCtGOyW5NXAA8Kn5LVGSJGlyLZyrQ1VtTHIUcDKwADihqs5JcmRrX966PhH4fFVdtsWqlSRJmjBzhimAqloJrBzZtnxk/UTgxPkqTJIkaVvgN6BLkiQNYJiSJEkawDAlSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYkiRJGsAwJUmSNIBhSpIkaQDDlCRJ0gCGKUmSpAEMU5IkSQMYpiRJkgYwTEmSJA1gmJIkSRrAMCVJkjSAYUqSJGkAw5QkSdIAhilJkqQBxgpTSQ5Kcl6SNUmOnqHPgUnOSnJOklPnt0xJkqTJtHCuDkkWAMuARwPrgFVJVlTVub0+twGOAw6qqh8mucMWqleSJGmijDMztR+wpqrOr6orgZOApSN9ngZ8vKp+CFBVP53fMiVJkibTOGFqV2Btb31d29Z3T+C2SU5JcmaSZ003UJIjkqxOsnrDhg2bV7EkSdIEGSdMZZptNbK+ENgX+EPgMcDfJrnn9e5UdXxVLamqJYsWLdrkYiVJkibNnNdM0c1E7dZbXwysn6bPRVV1GXBZki8BDwS+Oy9VSpIkTahxZqZWAXsl2SPJjsBhwIqRPp8CHpFkYZKdgP2Bb89vqZIkSZNnzpmpqtqY5CjgZGABcEJVnZPkyNa+vKq+neRzwNnA1cC7qupbW7JwSZKkSTDOaT6qaiWwcmTb8pH1NwFvmr/SJEmSJp/fgC5JkjSAYUqSJGkAw5QkSdIAhilJkqQBDFOSJEkDGKYkSZIGMExJkiQNYJiSJEkawDAlSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYkiRJGsAwJUmSNIBhSpIkaQDDlCRJ0gCGKUmSpAEMU5IkSQMYpiRJkgYYK0wlOSjJeUnWJDl6mvYDk/wyyVnt9ur5L1WSJGnyLJyrQ5IFwDLg0cA6YFWSFVV17kjXL1fV47ZAjZIkSRNrnJmp/YA1VXV+VV0JnAQs3bJlSZIkbRvGCVO7Amt76+vatlEPSfKNJJ9Nct/pBkpyRJLVSVZv2LBhM8qVJEmaLOOEqUyzrUbWvw7craoeCLwD+OR0A1XV8VW1pKqWLFq0aJMKlSRJmkTjhKl1wG699cXA+n6Hqrqkqi5tyyuBHZLsMm9VSpIkTahxwtQqYK8keyTZETgMWNHvkOROSdKW92vjXjzfxUqSJE2aOT/NV1UbkxwFnAwsAE6oqnOSHNnalwOHAi9IshG4HDisqkZPBUqSJG135gxTcM2pu5Uj25b3lo8Fjp3f0iRJkiaf34AuSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYkiRJGsAwJUmSNIBhSpIkaQDDlCRJ0gCGKUmSpAEMU5IkSQMYpiRJkgYwTEmSJA1gmJIkSRrAMCVJkjSAYUqSJGkAw5QkSdIAhilJkqQBDFOSJEkDGKYkSZIGGCtMJTkoyXlJ1iQ5epZ+v5vkqiSHzl+JkiRJk2vOMJVkAbAMOBjYG3hqkr1n6PdG4OT5LlKSJGlSjTMztR+wpqrOr6orgZOApdP0eyHwMeCn81ifJEnSRBsnTO0KrO2tr2vbrpFkV+CJwPL5K02SJGnyjROmMs22Gll/K/CKqrpq1oGSI5KsTrJ6w4YNY5YoSZI0uRaO0WcdsFtvfTGwfqTPEuCkJAC7AI9NsrGqPtnvVFXHA8cDLFmyZDSQSZIkbXPGCVOrgL2S7AH8CDgMeFq/Q1XtMbWc5ETg06NBSpIkaXs0Z5iqqo1JjqL7lN4C4ISqOifJka3d66QkSdKN1jgzU1TVSmDlyLZpQ1RVHT68LEmSpG2D34AuSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYkiRJGsAwJUmSNIBhSpIkaQDDlCRJ0gCGKUmSpAEMU5IkSQMYpiRJkgYwTEmSJA1gmJIkSRrAMCVJkjSAYUqSJGkAw5QkSdIAhilJkqQBDFOSJEkDGKYkSZIGGCtMJTkoyXlJ1iQ5epr2pUnOTnJWktVJHj7/pUqSJE2ehXN1SLIAWAY8GlgHrEqyoqrO7XX7ArCiqirJA4CPAPfeEgVLkiRNknFmpvYD1lTV+VV1JXASsLTfoaourapqq7cACkmSpBuBccLUrsDa3vq6tu06kjwxyXeAzwB/Mj/lSZIkTbZxwlSm2Xa9maeq+kRV3Rs4BHjttAMlR7RrqlZv2LBhkwqVJEmaROOEqXXAbr31xcD6mTpX1ZeAeyTZZZq246tqSVUtWbRo0SYXK0mSNGnGCVOrgL2S7JFkR+AwYEW/Q5I9k6QtPwjYEbh4vouVJEmaNHN+mq+qNiY5CjgZWACcUFXnJDmytS8HngQ8K8lvgcuBp/QuSJckSdpuzRmmAKpqJbByZNvy3vIbgTfOb2mSJEmTz29AlyRJGsAwJUmSNIBhSpIkaQDDlCRJ0gCGKUmSpAEMU5IkSQMYpiRJkgYwTEmSJA1gmJIkSRrAMCVJkjSAYUqSJGkAw5QkSdIAhilJkqQBDFOSJEkDGKYkSZIGMExJkiQNYJiSJEkawDAlSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBhgrTCU5KMl5SdYkOXqa9qcnObvdTkvywPkvVZIkafLMGaaSLACWAQcDewNPTbL3SLcfAAdU1QOA1wLHz3ehkiRJk2icman9gDVVdX5VXQmcBCztd6iq06rq5231a8Di+S1TkiRpMo0TpnYF1vbW17VtM3ku8NnpGpIckWR1ktUbNmwYv0pJkqQJNU6YyjTbatqOySPpwtQrpmuvquOraklVLVm0aNH4VUqSJE2ohWP0WQfs1ltfDKwf7ZTkAcC7gIOr6uL5KU+SJGmyjTMztQrYK8keSXYEDgNW9DskuSvwceCZVfXd+S9TkiRpMs05M1VVG5McBZwMLABOqKpzkhzZ2pcDrwZuDxyXBGBjVS3ZcmVLkiRNhnFO81FVK4GVI9uW95afBzxvfkuTJEmafH4DuiRJ0gCGKUmSpAEMU5IkSQMYpiRJkgYwTEmSJA1gmJIkSRrAMCVJkjSAYUqSJGkAw5QkSdIAhilJkqQBDFOSJEkDGKYkSZIGMExJkiQNYJiSJEkawDAlSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYkiRJGmCsMJXkoCTnJVmT5Ohp2u+d5PQkVyR52fyXKUmSNJkWztUhyQJgGfBoYB2wKsmKqjq31+1nwIuAQ7ZEkZIkSZNqnJmp/YA1VXV+VV0JnAQs7Xeoqp9W1Srgt1ugRkmSpIk1TpjaFVjbW1/Xtm2yJEckWZ1k9YYNGzZnCEmSpIkyTpjKNNtqc3ZWVcdX1ZKqWrJo0aLNGUKSJGmijBOm1gG79dYXA+u3TDmSJEnblnHC1CpgryR7JNkROAxYsWXLkiRJ2jbM+Wm+qtqY5CjgZGABcEJVnZPkyNa+PMmdgNXArYCrk7wY2LuqLtlypUuSJG19c4YpgKpaCawc2ba8t3wh3ek/SZKkGxW/AV2SJGkAw5QkSdIAhilJkqQBDFOSJEkDGKYkSZIGMExJkiQNYJiSJEkawDAlSZI0gGFKkiRpAMOUJEnSAIYpSZKkAQxTkiRJAximJEmSBjBMSZIkDWCYkiRJGsAwJUmSNIBhSpIkaQDDlCRJ0gCGKUmSpAEMU5IkSQMYpiRJkgYwTEmSJA0wVphKclCS85KsSXL0NO1J8vbWfnaSB81/qZIkSZNnzjCVZAGwDDgY2Bt4apK9R7odDOzVbkcA/zrPdUqSJE2kcWam9gPWVNX5VXUlcBKwdKTPUuC91fkacJskd57nWiVJkibOwjH67Aqs7a2vA/Yfo8+uwI/7nZIcQTdzBXBpkvM2qVoNtQtw0dYuYluQNz97a5egzedxPq6/y9auQJvP43xMedG8Hed3m6lhnDA1XRW1GX2oquOB48fYp7aAJKurasnWrkPakjzOdWPgcT5ZxjnNtw7Yrbe+GFi/GX0kSZK2O+OEqVXAXkn2SLIjcBiwYqTPCuBZ7VN9DwZ+WVU/Hh1IkiRpezPnab6q2pjkKOBkYAFwQlWdk+TI1r4cWAk8FlgD/Bp4zpYrWQN4ilU3Bh7nujHwOJ8gqbrepU2SJEkak9+ALkmSNIBhSpIkaQDD1DYkyd8kOaf9yZ6zkuyf5IIku4xx3zsm+WCS85OcmeT0JE9sbUuSvH2MMU6bj8ehG6/NPYaTHJ5kQ7vP1G30LzGM3ufFSXaa30cgdZIcMtcxOE/7OTzJMdNs3z1JJXlhb9uxSQ7fhLGPSfKj9vP0rSRPmJ+qb3zG+Z4pTYAkDwEeBzyoqq5obz47jnnfAJ8E3lNVT2vb7gY8AaCqVgOr5xqnqh66edVLw47h5sNVddQm9H8x8H66D8WM1rKgqq7ahLGkUYcAnwbOHW1IsrCqNt4ANfwU+Isk/9b+Qsnm+JeqenOS+wBfTnKHqrp6qvEGfCzbNGemth13Bi6qqisAquqiqrrmu7yS3DzJ55I8f5r7/h5wZfvkJe3+/1tV72j3PTDJp9vyMUlOSHJKm8V6UW8fl26hx6YbhyHH8LTasXtKko8m+U6SD7SvaHkRcBfgv5L8V+t7aZLXJPlv4CFJXtJ+G/9Wkhe3Pru3cd7TZs8+mmSnJI9K8onefh+d5OPz8qxoIiR5RpIz2izNv7W/Szt13LwuyTeSfK3N8j+U7pfRN7X+92jH4euTnEoXcB6V5H+SfLP9n3rTNt4FSd7Y9nVGkj2T7JzkB0l2aH1u1frtMEfZG4AvANf7kw1J9mn1np3kE0luO9tAVfVtYCOwyzSP5cQkh/bGvrT9O+3PX2vbN8mp6c6EnJzt/E/MGaa2HZ8Hdkvy3STHJTmg13ZL4D+AD1bVO6e5732Br2/Cvu4NPIbu7zL+3Rg/0NI4hhzDAE/JdU/z3bxt/x26Wai9gbsDD6uqt9N9cfAjq+qRrd8tgG9V1f7A5XRf4bI/8GDg+Ul+p/W7F3B8VT0AuAT4M+CLwH2SLGp9ngO8e/OfCk2SNivzFLpjZx/gKuDprfkWwNeq6oHAl4DnV9VpdN+v+PKq2qeqvt/63qaqDgCWAScCT6mq+9OdBXpBb5eXVNV+wLHAW6vqV8ApwB+29sOAj1XVb8co/w3AS6fCX897gVe04/ibwN/N8RzsD1xNF9CueSxV9ZY59n+9n7/2nvEO4NCq2hc4AXjdGI9lm2WY2kZU1aXAvnR/23AD8OFce278U8C7q+q944yVZFn7LWvVDF0+U1VXVNVFdNPIdxxWvTQvx/CH2xvX1O3ytv2MqlrXTk2cBew+w/2vAj7Wlh8OfKKqLmt1fRx4RGtbW1VfbcvvBx5e3XfIvA94RpLbAA8BPjvmQ9fkexTdsbkqyVlt/e6t7Uq603kAZzLz8QXw4fbvvYAfVNV32/p7gP/T6/eh3r8Pacvv4trvaBw7rFfVD4AzgKdNbUtya7owdOoM++/7y/aY30wX/qa+L+nDM/QfNd3P372A+wH/2cZ+Fd1fRtluec3UNqRd43EKcEqSb3Lt1O5XgYOTfLCqKsmfA1OnSh4LnAM8qTfOn6e7XmWm66Su6C1fhceJ5smAY3g24x6vv+ldJzXbXz4d/fK9qfV3082e/Qb4d68j2a6E7prSV07T9ttewJjr/8PLeuPNpkaXq+qr7TTzAcCCqvrWGHVPeT3wUbqZs031L1X15mm2X9Zb3kibfGmn8frXOk738xfgnKp6CDcSzkxtI5LcK8levU37AP/bll8NXAwcB1BVy3q/va+nO0VxsyT9aWY/5aQb1MBjeHP8Cth5hrYvAYe066FuATwR+HJru2u6i+UBngp8pdW0nu7U4avoTuFo+/EF4NAkdwBIcrt0H9KZzWzH13eA3ZPs2dafCZzaa39K79/Te9vfSzdbtUmnkKvqO3QXwj+urf8S+HmSqdnW0f1vqgvoZu4AlgJzXfpxHrBo6ucoyQ5J7jtg/xPPMLXtuCXwniTnJjmb7vz0Mb32F9MFpn8avWP7reoQ4IB2keMZdNO+r9jSRUs9m30MN6PXTM316dLjgc+mXYDeV1VfpwtEZwD/Dbyrqv6nNX8beHar8XbAv/bu+gG604DX+wSXtl3t9XwV8Pn2uv8n3QcmZnMS8PJ2kfk9Rsb7Dd2pun9vM7BXA8t7XW6a7oMQfwH8ZW/7B4Dbcu1pwE3xOq57Ku3ZdBfIn033i8trNmPMKe+ke/84g+46w8tm69w+WXgo8MYk36A7/bddfxrcPycjSU2S3YFPV9X9Zmg/Fvifqvq/N2hh2m4kuQBY0q5JHW07FFhaVc/sbTsc2L2qjrmhatSm81oYSRpDkjPpfiN/6dauRdufJO8ADmbuawQ1gZyZkiRpQiXZh+6Teads5VI0C8OUJEnSAF6ALkmSNIBhSpIkaQDDlCRJ0gCGKUmSpAEMU5IkSQP8f/AdA94IGmTzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Comparing with sklearn-default-Gini. \n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "sk_tree_pred_val = clf.predict(X_val)\n",
    "\n",
    "sk_tree_score = accuracy_score(Y_val,sk_tree_pred_val )\n",
    "sk_tree_pred_test = clf.predict(X_test)\n",
    "sk_tree_gini_test=accuracy_score(Y_test, sk_tree_pred_test)\n",
    "\n",
    "# Print validation and test-results. \n",
    "print(\"|GINI|\")\n",
    "print(\"SciKit-Learn-Gini Validation Score = \",sk_tree_score)\n",
    "print(\"SciKit-Learn-Gini Test Score = \",sk_tree_gini_test)\n",
    "\n",
    "# Comparing with sklearn entropy. \n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "sk_tree_pred_val = clf.predict(X_val)\n",
    "sk_tree_score = accuracy_score(Y_val,sk_tree_pred_val )\n",
    "sk_tree_pred_test = clf.predict(X_test)\n",
    "sk_tree_score_entropy_test=accuracy_score(Y_test,sk_tree_pred_test)\n",
    "\n",
    "# Print validation and test-results. \n",
    "print(\"\\n|ENTROPY|\")\n",
    "print(\"SciKit-Learn-Entropy Validation Score = \",sk_tree_score)\n",
    "print(\"SciKit-Learn-Entropy Test Score = \",sk_tree_score_entropy_test)\n",
    "\n",
    "print(\"\\n|COMMENT|\")\n",
    "print(\"Test score for Sklearn was found to be \",max(sk_tree_score_entropy_test,sk_tree_gini_test) , \"VS my algorithm, with a test score of \", best_model_score)\n",
    "\n",
    "sk_names =[\"Sk-Gini\", \"Sk-Entropy\", best_model_name]\n",
    "\n",
    "# Create a barplot. \n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(x = sk_names, y = [sk_tree_gini_test,sk_tree_score_entropy_test, best_model_score])\n",
    "ax.set_title(\"Test-Set Score Comparison With Sk-Learn Algorithms.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9648fb51b99fc0f93c7fc244fc6c68c2e06972849eab94bfa9b40f6e3c134e75"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
